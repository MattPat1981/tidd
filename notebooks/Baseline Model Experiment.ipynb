{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model Experiment \n",
    "\n",
    "This notebook contains a baseline model experiment with a simple Artificial Neural Network (ANN) using [FastAI Tabular Learner](https://docs.fast.ai/tabular.learner.html). We begin experiments with this simple model on the premise that - when taking into account the level of effort - clever feature engineering typically outperforms employing alternative model types (such as using LSTMs). \n",
    "\n",
    "A few items about the experiment: \n",
    "\n",
    "- We train for `D` days (periods) and predict on the next day in order to train the model. After each complete period, the model is retrained on the new data (either from scratch or with transfter learning). We model the experiment this way to coincide with how a model would be used in practice and to account for the fast that the most recent data has an outsized influence on model performance. \n",
    "- We identify and utilize periods of observations for model training and testing that are greater than some threshold `period_size`. Deep neural networks cannot be trained with missing data. \n",
    "- Related to the above, it would be an inappropriate training strategy to simply concatenate the periods together during the training process - the value(s) at the end of a period does not help predict the values at the beginning of the next period. As such, we will need to retrain the model for each subsequent day in the training set using transfer learning. The same testing set is used throughout the model training process. \n",
    "- An appropriate learning rate is determined using an automated learning rate finder. \n",
    "- The model is trained using an early stopping criterion that is intened to ovoid model overfitting. \n",
    "- A selection of possible model architectures for the ANN (nodes in the layers, drop out rates, etc). is available for the experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vconstan/.conda/envs/tsunami/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys; sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular import *\n",
    "from fastai.metrics import *\n",
    "from fastai import torch_core\n",
    "from fastai.callbacks import *\n",
    "from fastai.callbacks.mem import PeakMemMetric\n",
    "from fastai.utils.mod_display import *\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import plotly.graph_objects as go\n",
    "from PyNomaly import loop\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import seaborn as sns\n",
    "from scipy import spatial, stats\n",
    "from src import data\n",
    "import time\n",
    "import torch\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Parameters\n",
    "\n",
    "First, we can specify and select a variety of architectures for the model. More specifically, the below architectures are used by FastAI's tabular learner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = {\n",
    "    1: {\n",
    "        \"layers\": [10, 500, 1000, 2500, 25000, 2500, 1000, 500, 10],\n",
    "        \"ps\": [0.0, 0.1, 0.2, 0.2, 0.25, 0.2, 0.2, 0.1, 0.0]\n",
    "    },\n",
    "    2: {\n",
    "        \"layers\": [10, 50, 100, 250, 1000, 250, 100, 50, 10],\n",
    "        \"ps\": [0.5, 0.4, 0.3, 0.2, 0.2, 0.1, 0.05, 0.025, 0.0]\n",
    "    },\n",
    "    3: {\n",
    "        \"layers\": [10, 50, 100, 250, 1000, 250, 100, 50, 10],\n",
    "        \"ps\": [0.0, 0.025, 0.05, 0.1, 0.2, 0.2, 0.3, 0.4, 0.5]\n",
    "    },\n",
    "    4: {\n",
    "        \"layers\": [50000, 5000, 1000, 500, 25, 1],\n",
    "        \"ps\": [0.2, 0.15, 0.1, 0.05, 0.025, 0.]\n",
    "    },\n",
    "    5: {\n",
    "        \"layers\": [50000, 7500, 2500, 1000, 250, 80, 25, 1],\n",
    "        \"ps\": [0.25, 0.2, 0.15, 0.1, 0.05, 0.025, 0.01, 0]\n",
    "    },\n",
    "    6: {\n",
    "        \"layers\": [1000, 250, 50, 1],\n",
    "        \"ps\": [0.2, 0.1, 0.025, 0.]\n",
    "    },\n",
    "    7: {\n",
    "        \"layers\": [3, 3],\n",
    "        \"ps\": [0.1, 0.0]\n",
    "    },\n",
    "    8: {\n",
    "        \"layers\": [10, 10, 10, 10, 10, 10],\n",
    "        \"ps\": [0.1, 0.1, 0.1, 0.1, 0.05, 0.0]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other experimental parameters are set below, such as which data to use in the experiment, which satellite to model, and other model specifications. A single model is applied to each satellite, so the below would be repeated over more than one satellite to conduct a thorough experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(object): \n",
    "    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 dependent_variable: str, \n",
    "                 independent_variables: list, \n",
    "                 satellites: list, \n",
    "                 model: tabular_learner, \n",
    "                 year: int = 2012, \n",
    "                 location: str = \"hawaii\", \n",
    "                 elevation_filter: Union[int, None] = None, \n",
    "                 time_aggregation: str = \"1min\", \n",
    "                 batch_normalization: bool = True, weight_decay: float = 0.1, \n",
    "                 model_save_directory: str = \"\", model_name: str = \"model-latest\",\n",
    "                 max_epochs: int = 500, cuda_device: int = 0):\n",
    "        \n",
    "        # experiment setup\n",
    "        self.year = year\n",
    "        self.location = location\n",
    "        # TODO: below path will need rework when placed in src/modeling.py\n",
    "        self.data_paths = Path('../data/' + LOCATION + '/' + str(YEAR))\n",
    "        self.days = [str(f).split(\"/\")[-1] for f in self.data_paths.iterdir() if f.is_dir()]\n",
    "        self.satellites = satellites\n",
    "        self.elevation_filter = elevation_filter\n",
    "        self.time_aggregation = time_aggregation, \n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.weight_decay = weight_decay\n",
    "        self.model_save_directory = model_save_directory\n",
    "        self.model_name = model_name\n",
    "        self.max_epochs = max_epochs\n",
    "        self.cuda_device = cuda_device\n",
    "        self.model = model\n",
    "        # TODO: important note that the data as needed for the model should be passed \n",
    "        self.data = data\n",
    "        \n",
    "        # TODO: experiment results \n",
    "        \n",
    "    def set_data(self) -> None: \n",
    "        \n",
    "        # read all of the data into a dictionary \n",
    "        dataframes = dict()\n",
    "        for d in DAYS:\n",
    "            print(\"\\n--- \" + str(d) + \"---\")\n",
    "\n",
    "            # read in the data \n",
    "            df = data.read_day(\n",
    "                location=LOCATION,\n",
    "                year=YEAR,\n",
    "                day_of_year=int(d)\n",
    "            )\n",
    "            dataframes[d] = df\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "This experiment uses data from the Hawaii dataset. We will train the model on a period of `D` days (periods) and test on the latest day (period). The day of the earthquake will represent the _validation_ set, data that is unseen during the model training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DAYS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9a622fa6086c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDAYS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# read in the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DAYS' is not defined"
     ]
    }
   ],
   "source": [
    "dataframes = dict()\n",
    "for d in DAYS:\n",
    "    print(\"\\n--- \" + str(d) + \"---\")\n",
    "    \n",
    "    # read in the data \n",
    "    df = data.read_day(\n",
    "        location=LOCATION,\n",
    "        year=YEAR,\n",
    "        day_of_year=int(d)\n",
    "    )\n",
    "    dataframes[d] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the dataframes loaded previously into one large dataframe \n",
    "df_all = pd.concat([dataframes[d] for d in dataframes.keys()]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Dataset\n",
    "\n",
    "The above experimental parameter definitions will be interesting to explore. However, we also need to be smart about which data we feed into the model for training and how to feet that data into the model as well in order to set a good baseline for our experiments. \n",
    "\n",
    "The data contains missing values and - in practice and in the real-world - and we'll need to account for that in our modeling strategy. We'll want to train the model on each chunk of data to come in, and a chunk represents a continuous stretch of data without any missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat = df_all.filter(regex=GROUND_STATION + \"__\" + SAT, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_sat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_sat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5842b9d7dfb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_sat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_sat' is not defined"
     ]
    }
   ],
   "source": [
    "df_sat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elevation Filter \n",
    "\n",
    "Exclude any observations recorded below a specific elevation specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ELE_FILTER is not None: \n",
    "    df_sat = df_sat[df_sat[GROUND_STATION + \"__\" + SAT + \"_ele\"] > ELE_FILTER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_sat.dropna().resample(TIME_AGG).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Any Stray Outliers \n",
    "\n",
    "While the elevation filtering does help to provide clean, consistent data to the model, it is a possibility that the connection between the ground station and the satellite can sometimes cause inconsistent / noisy TEC estimates. These are not indicative of normal behavior, so we will want to remove them from our dataset. \n",
    "\n",
    "This behavior **only occurs for the G04 satellite**, so we will want to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a density-based local outlier approach to identify and later remove these stray values from our training set. This should improve our results and simply the model training task to some degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_index = df_sat.index.to_series().between('2012-10-22', '2012-10-23 23:59:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat_outliers = df_sat[filter_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = loop.LocalOutlierProbability(\n",
    "    df_sat_outliers[\n",
    "        [\n",
    "            GROUND_STATION + \"__\" + SAT\n",
    "        ]\n",
    "    ], \n",
    "    extent=3, \n",
    "    n_neighbors=500\n",
    ").fit()\n",
    "scores = m.local_outlier_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat_outliers[\"outlier_scores\"] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subplots similar to the paper\n",
    "values = list()\n",
    "for val in df_sat_outliers:\n",
    "    if val != 'outlier_scores':\n",
    "        values.append(val)\n",
    "\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "f, axes = plt.subplots(len(values), 1, figsize=(25,25), sharex=True)\n",
    "i = 0\n",
    "\n",
    "for val in values:\n",
    "    ax = sns.lineplot(x=df_sat_outliers.index, y=val, ax=axes[i], data=df_sat_outliers, color=\"gray\")\n",
    "    ax.lines[0].set_linestyle(\"--\")\n",
    "    \n",
    "    ax2 = sns.scatterplot(x=df_sat_outliers.index, y=val,\n",
    "                data=df_sat_outliers, ax=axes[i],\n",
    "                hue=\"outlier_scores\", \n",
    "                palette=\"bwr\",\n",
    "                legend=False)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red dots above indicate points which may be considered as more likely to be anomalous by the Local Outlier Probability (LoOP) outlier detection approach. Our strategy will be to remove any values from the data that are close to these detected points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVAL_WINDOW = 10 # number of minutes / points before and after \n",
    "OUTLIER_THRESHOLD = 0.9 # on a scale [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each value in df_sat_outliers that matches the threshold\n",
    "# identify the idx and do the thing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = df_sat_outliers[df_sat_outliers[\"outlier_scores\"] > OUTLIER_THRESHOLD]\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removal_windows = list()\n",
    "list_index = list(df_sat_outliers.index.values)\n",
    "for idx in outliers.index.values:\n",
    "    idx_i = list_index.index(idx)\n",
    "    idx_range = [idx_i - REMOVAL_WINDOW, idx_i + REMOVAL_WINDOW]\n",
    "    removal_windows.append(idx_range)\n",
    "for rw in removal_windows:\n",
    "    df_sat_outliers.iloc[rw[0]:rw[1], :] = None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subplots similar to the paper\n",
    "values = list()\n",
    "for val in df_sat_outliers:\n",
    "    if val != 'outlier_scores':\n",
    "        values.append(val)\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "f, axes = plt.subplots(len(values), 1, figsize=(25,25), sharex=True)\n",
    "i = 0\n",
    "\n",
    "for val in values:\n",
    "    ax = sns.lineplot(x=df_sat_outliers.index, y=val, ax=axes[i], data=df_sat_outliers, color=\"gray\")\n",
    "    ax.lines[0].set_linestyle(\"--\")\n",
    "    \n",
    "    ax2 = sns.scatterplot(x=df_sat_outliers.index, y=val,\n",
    "                data=df_sat_outliers, ax=axes[i],\n",
    "                color=\"blue\",\n",
    "                legend=False)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are comfortable with our outlier removal approach, let's apply it to the modeling dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removal_windows = list()\n",
    "list_index = list(df_sat.index.values)\n",
    "for idx in outliers.index.values:\n",
    "    idx_i = list_index.index(idx)\n",
    "    idx_range = [idx_i - REMOVAL_WINDOW, idx_i + REMOVAL_WINDOW]\n",
    "    removal_windows.append(idx_range)\n",
    "for rw in removal_windows:\n",
    "    df_sat.iloc[rw[0]:rw[1], :] = None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data \n",
    "YEAR = 2012 # can be 2012, 2015\n",
    "LOCATION = \"hawaii\" # can be hawaii, chile\n",
    "DATA_PATHS = Path('../data/' + LOCATION + '/' + str(YEAR))\n",
    "DAYS = [str(f).split(\"/\")[-1] for f in DATA_PATHS.iterdir() if f.is_dir()]\n",
    "GROUND_STATION = \"ahup\" # string\n",
    "SAT = \"G07\" # string\n",
    "ELE_FILTER = 35 # int or None\n",
    "TIME_AGG = \"1Min\"\n",
    "BATCH_NORM = True\n",
    "WEIGHT_DECAY = 0.1\n",
    "USE_MISSING_INDICATOR = False # create features that inform the model of missing data \n",
    "\n",
    "# model specification \n",
    "MODEL_SAVE_DIR = \"\"\n",
    "MODEL_NAME = \"model-latest\"\n",
    "if MODEL_SAVE_DIR == \"\":\n",
    "    MODEL_LOCATION = MODEL_NAME\n",
    "else:\n",
    "    MODEL_LOCATION = MODEL_SAVE_DIR + \"/\" + MODEL_NAME\n",
    "MODEL_ARCHITECTURE = 7\n",
    "BATCH_SIZE = 16\n",
    "DEP = GROUND_STATION + \"__\" + SAT\n",
    "FEATURES = [\n",
    "#     GROUND_STATION + \"__\" + SAT,\n",
    "    GROUND_STATION + \"__\" + SAT + \"_ele\",\n",
    "#     GROUND_STATION + \"__\" + SAT + \"_lat\",\n",
    "#     GROUND_STATION + \"__\" + SAT + \"_lon\",\n",
    "#     GROUND_STATION + \"__\" + SAT + \"_h_ipp\"\n",
    "]\n",
    "MAX_EPOCHS = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a CUDA Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = torch.device('cuda:' + str(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful command for monitoring GPU utilization is one from `nvidia-smi`:\n",
    "\n",
    "```bash\n",
    "nvidia-smi -q -g 0 -d UTILIZATION -l\n",
    "```\n",
    "\n",
    "Or: \n",
    "\n",
    "```bash\n",
    "gpustat -cp -i 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a Missing Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model[\"missing\"] = df_model.apply(\n",
    "    lambda row: [1 if any(row.isna()) else 0][0],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model[\"missing\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model[df_model[\"missing\"] == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model[df_model[\"missing\"] == 0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Available Data\n",
    "\n",
    "#### All Base Features for The Satellite and Ground Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subplots that show the values in the data\n",
    "values = list()\n",
    "for val in df_sat:\n",
    "    values.append(val)\n",
    "\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "f, axes = plt.subplots(len(values), 1, figsize=(100,25), sharex=True)\n",
    "i = 0\n",
    "\n",
    "for val in values:\n",
    "    ax = sns.lineplot(x=df_sat.index, y=val, ax=axes[i], data=df_sat, color=\"gray\")\n",
    "    ax.lines[0].set_linestyle(\"--\")\n",
    "    \n",
    "    ax2 = sns.scatterplot(x=df_sat.index, y=val,\n",
    "                data=df_sat, ax=axes[i],\n",
    "                color=\"blue\")\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the Day of the Earthquake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get those dates from the 28th of the month, day of the earthquake\n",
    "filter_index = df_sat.index.to_series().between('2012-10-28', '2012-10-28 23:59:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat_earthquake = df_sat[filter_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat_earthquake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subplots similar to the paper\n",
    "values = list()\n",
    "for val in df_sat_earthquake:\n",
    "    values.append(val)\n",
    "\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "f, axes = plt.subplots(len(values), 1, figsize=(25,25), sharex=True)\n",
    "i = 0\n",
    "\n",
    "for val in values:\n",
    "    ax = sns.lineplot(x=df_sat_earthquake.index, y=val, ax=axes[i], data=df_sat_earthquake, color=\"gray\")\n",
    "    ax.lines[0].set_linestyle(\"--\")\n",
    "    \n",
    "    ax2 = sns.scatterplot(x=df_sat_earthquake.index, y=val,\n",
    "                data=df_sat_earthquake, ax=axes[i],\n",
    "                color=\"blue\")\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split The Data Into Periods \n",
    "\n",
    "These periods are defined by consecutive empty (NaN) values in the dataframe. The data is only available for the satellite as it passes close to the ground station on each day. We will train the data in a similar way, ensuring our approach is compatible with the constraints in the operating environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(dataframe: pd.DataFrame) -> list: \n",
    "\n",
    "    # handle missing values and \"chunk\" the data for training and testing \n",
    "    events = np.split(dataframe, np.where(np.isnan(dataframe))[0])\n",
    "\n",
    "    # removing NaN entries\n",
    "    events = [ev[~np.isnan(ev)] for ev in events if not isinstance(ev, np.ndarray)]\n",
    "\n",
    "    # removing empty DataFrames\n",
    "    events = [ev.dropna() for ev in events if not ev.empty and ev.shape[0] > 100]\n",
    "\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = split_dataframe(df_model)\n",
    "len(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from domain knowledge that each period in the data corresponds to a specific day. The third day in the dataset (`302`) corresponds to the third period (index `2`) in the dataset. \n",
    "\n",
    "All of the experiments will attempt at detecting the anomaly using an analysis of the residual values using the day of the earthquake, which will be contained in the `validation` set. Data prior to the day of the earthquake, with the day prior to the earthquake being used for `test` data. Any days prior to that are considered as training data. \n",
    "\n",
    "At some point in the future, we will want to setup a controlled trail of how this would operate in practice (daily retraining of models). For now, we will focus on the task of training the model for the first time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the Data \n",
    "\n",
    "Deep learning models do not perform well when model inputs are not scaled appropriately. In this application, we will scale the data for each feature to a scale of -1 to 1, and do so separetely for each day. This is again driven by operational considerations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_events = list()\n",
    "for ev in events: \n",
    "    \n",
    "    # for each column in the data, rescale -1 to 1 \n",
    "    col_data = list()\n",
    "    for col in ev.columns.values:\n",
    "        \n",
    "        normalized_data = minmax_scale(\n",
    "                    ev[col].dropna(), \n",
    "                    feature_range=(-1, 1)\n",
    "                )\n",
    "        col_data.append(normalized_data)\n",
    "        \n",
    "    df_period = pd.DataFrame(np.array(col_data).T, columns=list(ev.columns.values) )\n",
    "    df_period[\"timestamp\"] = ev[col].index\n",
    "    df_period.index = df_period[\"timestamp\"]\n",
    "    df_period = df_period.drop(columns=[\"timestamp\"])\n",
    "    \n",
    "    # convert to seconds of the day for later annotation \n",
    "    df_period[\"sod\"] = (df_period.index.hour*60+df_period.index.minute)*60 + df_period.index.second\n",
    "    \n",
    "    if USE_MISSING_INDICATOR:\n",
    "        df_period[\"missing\"] = ev[\"missing\"]\n",
    "    \n",
    "    normalized_events.append(df_period)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(normalized_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_events[13].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_events[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocate Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_periods(periods: list, index_earthquake: int) -> dict:\n",
    "    \n",
    "    data = dict()\n",
    "    \n",
    "    data[\"valid\"] = periods[index_earthquake]\n",
    "    data[\"test\"] = periods[index_earthquake - 1]\n",
    "    data[\"train\"] = periods[0:index_earthquake - 1]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_model_by_period = allocate_periods(normalized_events, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_model_by_period[\"train\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function preps the data for modeling in Fast AI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Work in progress\n",
    "def make_dataBunch(df_train: pd.DataFrame, df_test: pd.DataFrame, df_valid: pd.DataFrame, features: list, dependent: str, include_catvars: bool = False, catvars: list = [], batch_size: int = 256):\n",
    "    \"\"\"\n",
    "    Creates a TabularDataBunch to feed as an input \n",
    "    into the learner. \n",
    "    \"\"\"\n",
    "    \n",
    "    valid_start_index = df_train.shape[0] + 1\n",
    "    valid_end_index = df_train.shape[0] + df_test.shape[0]\n",
    "    \n",
    "    df_train_validation = pd.concat([df_train, df_test])\n",
    "        \n",
    "    # create the data bunch\n",
    "    if include_catvars:\n",
    "        data = TabularDataBunch.from_df(\n",
    "            \"models\", \n",
    "            df_train_validation[features + [dependent]], \n",
    "            dependent, \n",
    "            valid_idx=np.array(list(range(valid_start_index, valid_end_index))),\n",
    "#             test_df=df_test, \n",
    "            procs=[Categorify],\n",
    "            bs=batch_size, # batch size\n",
    "            cat_names=catvars,\n",
    "            device=cuda_device,\n",
    "            num_workers=0\n",
    "        )\n",
    "    else:\n",
    "        data = TabularDataBunch.from_df(\n",
    "            \"models\", \n",
    "            df_train_validation[features + [dependent]], \n",
    "            dependent, \n",
    "            valid_idx=np.array(list(range(valid_start_index, valid_end_index))),\n",
    "#             test_df=df_test, \n",
    "            procs=None, # disable any automatic preprocessing\n",
    "            bs=batch_size, # batch size\n",
    "            device=cuda_device,\n",
    "            num_workers=0\n",
    "        )\n",
    " \n",
    "    return {\n",
    "        \"databunch\": data, \n",
    "        \"train\": df_train, \n",
    "        \"test\": df_test, \n",
    "        \"valid\": df_valid\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the Learning Rate\n",
    "\n",
    "We utilize an [automatic learning rate finder](https://forums.fast.ai/t/automated-learning-rate-suggester/44199/8) to determine the ideal learning rate automatically. While this approach does not always guarantee that the perfect learning rate is found, in practice we have found the approach to work well and has been quite stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_appropriate_lr(model:Learner, lr_diff:int = 15, loss_threshold:float = .05, adjust_value:float = 1, plot:bool = False) -> float:\n",
    "    #Run the Learning Rate Finder\n",
    "    model.lr_find(\n",
    "        end_lr=2.,\n",
    "        stop_div=False # continues through all LRs as opposed to auto stopping\n",
    "    )\n",
    "    \n",
    "    #Get loss values and their corresponding gradients, and get lr values\n",
    "    losses = np.array(model.recorder.losses)\n",
    "    assert(lr_diff < len(losses))\n",
    "    loss_grad = np.gradient(losses)\n",
    "    lrs = model.recorder.lrs\n",
    "    \n",
    "    #Search for index in gradients where loss is lowest before the loss spike\n",
    "    #Initialize right and left idx using the lr_diff as a spacing unit\n",
    "    #Set the local min lr as -1 to signify if threshold is too low\n",
    "    r_idx = -1\n",
    "    l_idx = r_idx - lr_diff\n",
    "    while (l_idx >= -len(losses)) and (abs(loss_grad[r_idx] - loss_grad[l_idx]) > loss_threshold):\n",
    "        local_min_lr = lrs[l_idx]\n",
    "        r_idx -= 1\n",
    "        l_idx -= 1\n",
    "\n",
    "    lr_to_use = local_min_lr * adjust_value\n",
    "    \n",
    "    if plot:\n",
    "        # plots the gradients of the losses in respect to the learning rate change\n",
    "        plt.plot(loss_grad)\n",
    "        plt.plot(len(losses)+l_idx, loss_grad[l_idx],markersize=10,marker='o',color='red')\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xlabel(\"Index of LRs\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(np.log10(lrs), losses)\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xlabel(\"Log 10 Transform of Learning Rate\")\n",
    "        loss_coord = np.interp(np.log10(lr_to_use), np.log10(lrs), losses)\n",
    "        plt.plot(np.log10(lr_to_use), loss_coord, markersize=10,marker='o',color='red')\n",
    "        plt.show()\n",
    "        \n",
    "    return lr_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the model for each day in the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time-Series CV results will be tracked. Will contain the following: \n",
    "\n",
    "- `root_mean_square_error`\n",
    "- `period`\n",
    "- `learn_rate` \n",
    "- `training_time_seconds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_series_cv_log = list()\n",
    "cv_log_cols = [\"root_mean_square_error\", \"period\", \"learn_rate\", \"training_time_seconds\"]\n",
    "\n",
    "# learning_rates = [\n",
    "#     0.001,\n",
    "#     0.0005,\n",
    "#     0.0001,\n",
    "#     0.00005,\n",
    "#     0.00001,\n",
    "#     0.000005,\n",
    "#     0.000001,\n",
    "#     0.0000005,\n",
    "#     0.0000001,\n",
    "#     0.00000005,\n",
    "#     0.00000001\n",
    "# ]\n",
    "\n",
    "\n",
    "lr = None\n",
    "# for i in range(0, len(df_model_by_period[\"train\"])):\n",
    "    \n",
    "#     print(\"\\n---------- PERIOD: \" + str(i) + \" ----------\")\n",
    "    \n",
    "# get the current time \n",
    "now = time.time()\n",
    "\n",
    "#     # TODO: TEST THE BELOW\n",
    "#     # if this isn't the first time a model is trained, load it from memory \n",
    "#     if i != 0:\n",
    "#         # load learner\n",
    "#         lr = lr.load('model-latest')  \n",
    "\n",
    "# first, create a data bunch for this round of the modeling process        \n",
    "\n",
    "# below line \"hacked\" to use dataset as one large one \n",
    "#     df_train = df_model_by_period[\"train\"][i]\n",
    "\n",
    "\n",
    "\n",
    "# df_train = pd.concat(df_model_by_period[\"train\"])\n",
    "if USE_MISSING_INDICATOR is True:\n",
    "    df_train = pd.concat(df_model_by_period[\"train\"]).resample(\"1Min\").mean()\n",
    "    # fill with mean everywhere everywhere \n",
    "    for col in df_train.columns.values:\n",
    "                \n",
    "        df_train[col] = df_train[col].fillna(np.mean(df_train[col].dropna().values))\n",
    "\n",
    "    FEATURES += [\"missing\"]\n",
    "    \n",
    "else:\n",
    "    df_train = pd.concat(df_model_by_period[\"train\"])\n",
    "        \n",
    "# print(df_train.sample(frac=1).head(10))\n",
    "\n",
    "df_train[DEP + \"_target\"] = df_train[DEP]\n",
    "df_test = df_model_by_period[\"test\"]\n",
    "df_test[DEP + \"_target\"] = df_test[DEP]\n",
    "df_valid = df_model_by_period[\"valid\"]\n",
    "df_valid[DEP + \"_target\"] = df_valid[DEP]\n",
    "\n",
    "data_bunch = make_dataBunch(\n",
    "    df_train, \n",
    "    df_test,\n",
    "    df_valid,\n",
    "    FEATURES, \n",
    "    DEP + \"_target\", \n",
    "    include_catvars=False, \n",
    "    catvars=None, \n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# define the model \n",
    "if lr is None: \n",
    "    lr = tabular_learner(\n",
    "        data_bunch[\"databunch\"], \n",
    "        layers=architectures[MODEL_ARCHITECTURE][\"layers\"], \n",
    "        ps=architectures[MODEL_ARCHITECTURE][\"ps\"],\n",
    "        metrics=[root_mean_squared_error], \n",
    "        callback_fns=[CSVLogger, PeakMemMetric],\n",
    "        use_bn=BATCH_NORM,\n",
    "        wd=WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "#     # automatically find the ideal learning rate \n",
    "#     try:\n",
    "#         learn_rate = find_appropriate_lr(\n",
    "#             model=lr,\n",
    "#             plot=True\n",
    "#         )\n",
    "\n",
    "#     except:\n",
    "#         learn_rate = 0.001\n",
    "#         print(\"ERROR: cannot determine learning rate.\")\n",
    "\n",
    "# learn_rate = learning_rates[i]\n",
    "learn_rate = 0.0001\n",
    "\n",
    "print(\"Learning rate: \" + str(learn_rate))\n",
    "\n",
    "# train the model \n",
    "lr.fit_one_cycle(\n",
    "    MAX_EPOCHS, # max epochs\n",
    "    learn_rate,\n",
    "    callbacks=[\n",
    "        SaveModelCallback(\n",
    "            lr, \n",
    "            every='epoch', \n",
    "            monitor=['accuracy', 'root_mean_square_error']\n",
    "        ),\n",
    "        EarlyStoppingCallback(\n",
    "            lr,\n",
    "            monitor='valid_loss', #'valid_loss', 'root_mean_square_error'\n",
    "            min_delta=0.0001, # 0.0001\n",
    "            patience=15\n",
    "        ),\n",
    "        ShowGraph(\n",
    "            lr\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# loaded learners do not have a recorder\n",
    "lr.recorder.plot_losses()\n",
    "lr.recorder.plot_metrics()\n",
    "\n",
    "# last set of scores for this training cycle \n",
    "rmse = lr.recorder.metrics[-1][0].item()\n",
    "\n",
    "# get the end time \n",
    "later = time.time()\n",
    "\n",
    "# get time time difference in seconds\n",
    "time_diff = int(later - now)\n",
    "\n",
    "time_series_cv_log.append(\n",
    "#     [rmse, i, learn_rate, time_diff]\n",
    "    [rmse, 0, learn_rate, time_diff]\n",
    ")\n",
    "\n",
    "# save learner and export the model weights\n",
    "lr.save(MODEL_LOCATION)\n",
    "lr.export(MODEL_LOCATION + '-export.pkl')\n",
    "    \n",
    "#     # destroy the learner and clear the mem cache \n",
    "#     lr.destroy()\n",
    "#     torch.cuda.empty_cache() \n",
    "        \n",
    "print(\"Results from last training set.\")\n",
    "print(lr.show_results())\n",
    "   \n",
    "time_series_cv_results_df = pd.DataFrame(\n",
    "    time_series_cv_log,\n",
    "    columns=cv_log_cols\n",
    ")               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# destroy the learner and clear the mem cache\n",
    "# lr.destroy()\n",
    "# torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_cv_results_df\n",
    "# TODO: add num epochs trained \n",
    "# TODO: add total training time for period "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above training process is specific to a satellite and ground station. In order to report many of the metrics below, we will need to (at a later date) perform the training and testing process over multiple satellite and ground station combinations. \n",
    "\n",
    "Now that the model training is complete, load the learner from disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = tabular_learner(\n",
    "#         data_bunch[\"databunch\"], \n",
    "#         layers=architectures[MODEL_ARCHITECTURE][\"layers\"], \n",
    "#         ps=architectures[MODEL_ARCHITECTURE][\"ps\"],\n",
    "#         metrics=[root_mean_squared_error], \n",
    "#         callback_fns=[CSVLogger, PeakMemMetric],\n",
    "#         use_bn=BATCH_NORM,\n",
    "#         wd=WEIGHT_DECAY\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load learner\n",
    "# lr = lr.load('model-latest') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show me a summary look at the results \n",
    "lr.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the values on the testing set used for model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict values using input data from new data\n",
    "\n",
    "def predict_values(dataframe, learner, dependent, frac=1.0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Using the passed learner, predicts the appropriate value given the input data \n",
    "    and generates errors for analysis. \n",
    "    \"\"\"\n",
    "    \n",
    "    # get a sample of the dataset \n",
    "    dataframe_pred = dataframe.copy()\n",
    "    n_obs = int(dataframe_pred.shape[0] * frac)\n",
    "    idx = random.sample(range(0, dataframe_pred.shape[0]), n_obs)\n",
    "    dataframe_pred = dataframe_pred.iloc[idx, :]\n",
    "    \n",
    "    # get the predictions\n",
    "    \n",
    "    # TODO: update this to work with gpu over large samples like the other function \n",
    "    predictions = []\n",
    "    print('Generating predictions and errors...')\n",
    "    for idx, row in tqdm(dataframe_pred.iterrows(), total=dataframe_pred.shape[0]):        \n",
    "        predictions.append(learner.predict(row)[1].numpy()[0])\n",
    "    \n",
    "    \n",
    "    dataframe_pred[\"predicted\"] = predictions\n",
    "    dataframe_pred['error'] = dataframe_pred['predicted'] - dataframe_pred[dependent]\n",
    "    dataframe_pred['absolute_error'] = np.abs(dataframe_pred['error'])\n",
    "    dataframe_pred[\"timestamp\"] = dataframe_pred.index\n",
    "    \n",
    "    return dataframe_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a dataframe from the predicted values on the `test` set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = predict_values(\n",
    "    dataframe=data_bunch[\"test\"], # TODO: currently using latest databunch from prev loop\n",
    "    learner=lr, # TODO: currently using latest learner from prev loop\n",
    "    dependent=DEP + \"_target\",\n",
    "    frac=1.\n",
    ").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the predicted (blue) versus the actual (gray) values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 10))\n",
    "\n",
    "ax = sns.lineplot(x=df_test.index, y=DEP + \"_target\", data=df_test, color=\"gray\")\n",
    "ax.lines[0].set_linestyle(\"--\")\n",
    "\n",
    "ax2 = sns.scatterplot(x=df_test.index, y=DEP + \"_target\",\n",
    "            data=df_test,\n",
    "            color=\"gray\")\n",
    "\n",
    "\n",
    "ax3 = sns.lineplot(x=df_test.index, y=\"predicted\", data=df_test, color=\"blue\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the absolute error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 10))\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x=df_test.index, y=\"absolute_error\", data=df_test, color=\"red\")\n",
    "# ax.lines[0].set_linestyle(\"--\")\n",
    "\n",
    "# ax2 = sns.scatterplot(x=df_assess_day.index, y=\"absolute_error\",\n",
    "#             data=df_assess_day,\n",
    "#             color=\"darkgray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple moving average \n",
    "df_test[\"absolute_error_sma_3\"] = df_test[\"absolute_error\"].rolling(3, min_periods=1).mean()\n",
    "df_test[\"absolute_error_sma_5\"] = df_test[\"absolute_error\"].rolling(5, min_periods=1).mean()\n",
    "df_test[\"absolute_error_sma_10\"] = df_test[\"absolute_error\"].rolling(10, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 10))\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x=df_test.index, y=\"absolute_error\", data=df_test, color=\"gray\")\n",
    "ax = sns.lineplot(x=df_test.index, y=\"absolute_error_sma_3\", data=df_test, color=\"red\")\n",
    "# ax.lines[0].set_linestyle(\"--\")\n",
    "\n",
    "# ax2 = sns.scatterplot(x=df_assess_day.index, y=\"absolute_error\",\n",
    "#             data=df_assess_day,\n",
    "#             color=\"darkgray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify and Classify Tsunami-Related Disturbances\n",
    "\n",
    "Using the validation data - the day of the earthquake - use the residuals to detect the anomaly. \n",
    "\n",
    "This section is a **work in progress**. In order to estimate the model's `accuracy`, `recall`, `precision`, `F-score`, and `coverage`, we need to be able to classify specific time periods as anomalous - in this context, as ionoshperic disturbances related to tsunami waves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assess = predict_values(\n",
    "    dataframe=data_bunch[\"valid\"], # TODO: currently using latest databunch from prev loop\n",
    "    learner=lr, # TODO: currently using latest learner from prev loop\n",
    "    dependent=DEP + \"_target\",\n",
    "    frac=1.\n",
    ").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assess.sample(frac=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Actual and Predicted Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 10))\n",
    "\n",
    "ax = sns.lineplot(x=df_assess.index, y=DEP + \"_target\", data=df_assess, color=\"gray\")\n",
    "ax.lines[0].set_linestyle(\"--\")\n",
    "\n",
    "ax2 = sns.scatterplot(x=df_assess.index, y=DEP + \"_target\",\n",
    "            data=df_assess,\n",
    "            color=\"gray\")\n",
    "\n",
    "\n",
    "ax3 = sns.lineplot(x=df_assess.index, y=\"predicted\", data=df_assess, color=\"blue\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Absolute Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 10))\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x=df_assess.index, y=\"absolute_error\", data=df_assess, color=\"red\")\n",
    "# ax.lines[0].set_linestyle(\"--\")\n",
    "\n",
    "# ax2 = sns.scatterplot(x=df_assess_day.index, y=\"absolute_error\",\n",
    "#             data=df_assess_day,\n",
    "#             color=\"darkgray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we see a spike in the residual values at the approximate time of the tsunami-wave, we can use that information to explore ways of automating the detection of these disturbances. The approach to utilize for this classification is explored in the following sections. \n",
    "\n",
    "It should be noted that the ability to detect these disturbances must be thoroughly tested across multiple satellites and ground stations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Smoothed Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple moving average \n",
    "df_assess[\"absolute_error_sma_3\"] = df_assess[\"absolute_error\"].rolling(3, min_periods=1).mean()\n",
    "df_assess[\"absolute_error_sma_5\"] = df_assess[\"absolute_error\"].rolling(5, min_periods=1).mean()\n",
    "df_assess[\"absolute_error_sma_10\"] = df_assess[\"absolute_error\"].rolling(10, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 10))\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x=df_assess.index, y=\"absolute_error\", data=df_assess, color=\"gray\")\n",
    "ax = sns.lineplot(x=df_assess.index, y=\"absolute_error_sma_3\", data=df_assess, color=\"red\")\n",
    "# ax.lines[0].set_linestyle(\"--\")\n",
    "\n",
    "# ax2 = sns.scatterplot(x=df_assess_day.index, y=\"absolute_error\",\n",
    "#             data=df_assess_day,\n",
    "#             color=\"darkgray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Time Periods As Anomalous\n",
    "\n",
    "Model performance will generally always be impacted towards the start of a period (day) due to having less available data (data less than the specified batch size) for making predictions. This impacts model performance (confidence) at the start of any period, and thus confidence in the model will be less at early stages (until about an hour of data is collected). \n",
    "\n",
    "Once the event occurs, it seems to represent a large-enough deviation from the previous day(s) (used for model training) to result in elevated absolute errors for each prediction by the model after the change begins to occur in the the TEC variation. We can use this emperical finding to create a method for detection. \n",
    "\n",
    "We will not be able to do any better than an on-time detection of the event (meaning, there will be some lag between the start of the event and the detection of the event). We can score the approach and generate metrics (described further below) based on some specified threshold of acceptability of detection, e.g. 5 minutes (or something else). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of start times for tsunami-wave induced pertubations by satellite\n",
    "# start time defined by second in day \n",
    "sod_annotations = {\n",
    "    \"G04\": 31400,\n",
    "    \"G07\": 31160,\n",
    "    \"G08\": 31900,\n",
    "    \"G10\": 29900,\n",
    "    \"G20\": 31150\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an improved version of an earlier plot showing absolute errors that highlights the period of increased model confidence / performance at the start of the period and also has a vertical line indicating the start time of the pertubation in TEC variation. This plot will also show the time period in which we classify as anomalous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sod_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 10))\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x=df_assess[\"sod\"], y=\"absolute_error\", data=df_assess, color=\"lightgray\")\n",
    "ax = sns.lineplot(x=df_assess[\"sod\"], y=\"absolute_error_sma_3\", data=df_assess, color=\"black\")\n",
    "ax.axvline(sod_annotations[SAT], color=\"blue\", linestyle=\"--\")\n",
    "plt.text(sod_annotations[SAT] + 80, np.max(df_assess[\"absolute_error\"].values) / 2, \"Start of Event\", rotation=90, verticalalignment='center', color=\"blue\")\n",
    "ax.add_patch(\n",
    "    patches.Rectangle(\n",
    "        (0, 0), \n",
    "        df_assess.iloc[BATCH_SIZE][\"sod\"],\n",
    "        np.max(df_assess[\"absolute_error\"].values),\n",
    "        color=\"gray\",\n",
    "        alpha=0.3\n",
    "    )\n",
    ")\n",
    "ax.set_title('Absolute Error - Day of Earthquake - Satellite ' + SAT)\n",
    "\n",
    "# ax2 = sns.lineplot(x=df_assess[\"sod\"], y=\"absolute_error\", data=df_assess, color=\"black\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal to develop approach of handling residual values for detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of abs errors \n",
    "np.mean(df_assess[\"absolute_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard devation\n",
    "np.std(df_assess[\"absolute_error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of Errors \n",
    "\n",
    "A lot of error handling approaches and work assume that the errors have Gaussian distributions. Let's visualize the distribution of values and do a formal test for normality. If we find that the distribution of errors is generally normal, we could use approaches that are suitable for that type of data distribution. If not, we should look at other approaches for handling the model errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of the errors\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.distplot(df_assess[\"absolute_error\"].values, kde=True, rug=True);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a QQ plot. This will provide a visual indication as to the normality of the data. If the points closely follow the diagonal line, then they closely fit the pattern we would expect from a Gaussian distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq = stats.probplot(df_assess[\"absolute_error\"].values, dist='lognorm', sparams=(1))\n",
    "x = np.array([qq[0][0][0], qq[0][0][-1]])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(x=qq[0][0], y=qq[0][1], mode='markers')\n",
    "fig.add_scatter(x=x, y=qq[1][1] + qq[1][0]*x, mode='lines')\n",
    "fig.layout.update(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normality Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapiro normality test\n",
    "stat, p = stats.shapiro(df_assess[\"absolute_error\"].values)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Sample looks Gaussian (fail to reject H0)')\n",
    "else:\n",
    "    print('Sample does not look Gaussian (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D’Agostino’s K^2  normality test\n",
    "stat, p = stats.normaltest(df_assess[\"absolute_error\"].values)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Sample looks Gaussian (fail to reject H0)')\n",
    "else:\n",
    "    print('Sample does not look Gaussian (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anderson-Darling normality test\n",
    "result = stats.anderson(df_assess[\"absolute_error\"].values)\n",
    "print('Statistic: %.3f' % result.statistic)\n",
    "p = 0\n",
    "for i in range(len(result.critical_values)):\n",
    "    sl, cv = result.significance_level[i], result.critical_values[i]\n",
    "    if result.statistic < result.critical_values[i]:\n",
    "        print('%.3f: %.3f, data looks normal (fail to reject H0)' % (sl, cv))\n",
    "    else:\n",
    "        print('%.3f: %.3f, data does not look normal (reject H0)' % (sl, cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a Time-based Distancing Approach \n",
    "\n",
    "For each observation, get the distances from each of the previous observations from _T_ steps prior (so the difference / distance in absolute errors). Then, log transform these distances to prioritize local anomalies (inspired from approach by Ian Colwell for MSL anomaly detection). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of previous time steps T to consider\n",
    "DIST_WINDOW = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dist_measure(dataframe: pd.DataFrame, window_size: int = 10, show_dist_plot: bool = False) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    Calculates the distance \n",
    "    \"\"\"\n",
    "    \n",
    "    # create a dataframe for the errors \n",
    "    df_error = dataframe[[\"sod\", \"absolute_error_sma_3\"]].iloc[window_size:].reset_index().sort_values(by=\"sod\")\n",
    "    \n",
    "    # create a dataframe to store distances between absolute errors\n",
    "    # more specifically, to store the mean distance from an observation \n",
    "    # to its previous N neighbors \n",
    "    df_error_dist = pd.DataFrame(list(), index=df_error[\"sod\"], columns=df_error[\"sod\"])\n",
    "    for idx, row in tqdm(df_error.iterrows(), total=df_error.shape[0]):\n",
    "    \n",
    "        # get the current absolute error \n",
    "        abs_error = row[\"absolute_error_sma_3\"]\n",
    "\n",
    "        # get the absolute error values for the previous N values\n",
    "        prev = dataframe.iloc[idx:idx+window_size] # note indexing to account for window size \n",
    "        \n",
    "        # get the distances \n",
    "        for idx2, row2 in prev.iterrows():\n",
    "            \n",
    "            # TODO: could apply a weighted average here \n",
    "            \n",
    "            dist = spatial.distance.euclidean(abs_error, row2[\"absolute_error_sma_3\"])    \n",
    "            # note log transformed values here :\n",
    "            df_error_dist.loc[row[\"sod\"], row2[\"sod\"]] = np.log(dist + 1.)\n",
    "#             df_error_dist.loc[row[\"sod\"], row2[\"sod\"]] = dist\n",
    "            df_error_dist.loc[row2[\"sod\"], row[\"sod\"]] = np.log(dist + 1.) \n",
    "#             df_error_dist.loc[row2[\"sod\"], row[\"sod\"]] = dist \n",
    "            \n",
    "    # optionally show a plot \n",
    "    if show_dist_plot is True:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        ax = sns.heatmap(df_error_dist.sort_index().sort_index(axis=1).fillna(0.))\n",
    "        plt.show()\n",
    "        \n",
    "    # use that distance matrix to get the min/max/mean distance in the last window  \n",
    "    # we only want to use observations with a full window size \n",
    "    counts = df_error_dist.count().reset_index().rename(columns={0: \"counts\"})\n",
    "    full_sods = counts[counts[\"counts\"] >= window_size][\"sod\"]\n",
    "    \n",
    "#     df_error_dist_flat = df_error_dist.loc[full_sods].mean(axis=1)\n",
    "    df_error_dist_flat = df_error_dist.loc[full_sods].min(axis=1)\n",
    "#     df_error_dist_flat = df_error_dist.loc[full_sods].max(axis=1)\n",
    "    df_error_dist_flat = df_error_dist_flat.reset_index().rename(columns={0: \"dist_measure\"})\n",
    "\n",
    "    return df_error_dist_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_dist_calc = calc_dist_measure(\n",
    "    df_test,\n",
    "    window_size=DIST_WINDOW,\n",
    "    show_dist_plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_dist_calc[\"dist_measure\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val(dataframe, row):\n",
    "    try:\n",
    "        return dataframe[dataframe[\"sod\"] == row][\"dist_measure\"].values[0]\n",
    "    except:\n",
    "        return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"dist_measure\"] = [0.] * df_test.shape[0]\n",
    "df_test[\"dist_measure\"] = df_test[\"sod\"].apply(\n",
    "    lambda x: get_val(df_test_dist_calc, x)\n",
    ")\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# df_test[\"dist_measure_scaled\"] = scaler.fit_transform([df_test[\"dist_measure\"].values]).T\n",
    "\n",
    "# df_test[\"dist_measure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_test[\"dist_measure\"].values, kde=True, rug=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"dist_measure\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_test = loop.LocalOutlierProbability(\n",
    "    df_test.iloc[BATCH_SIZE:][\"absolute_error\"].values, \n",
    "    extent=3, \n",
    "    n_neighbors=30\n",
    ").fit()\n",
    "m_test_scores = m_test.local_outlier_probabilities\n",
    "df_test[\"loop_scores\"] = list([0.] * BATCH_SIZE) + list(m_test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VMAX = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(40, 10))\n",
    "grid = plt.GridSpec(4, 8, wspace=0.4, hspace=0.3)\n",
    "\n",
    "plt.subplot(grid[1:3, 0:8])\n",
    "\n",
    "# ax1 = plt.figure(figsize=(30, 10))\n",
    "ax1 = sns.lineplot(x=df_test[\"sod\"], y=\"absolute_error\", data=df_test, color=\"black\")\n",
    "ax1.margins(x=0)\n",
    "\n",
    "# add a vertical line to indicate when a full window of distance values is available \n",
    "ax1.axvline(df_test.iloc[DIST_WINDOW][\"sod\"], color=\"purple\", linestyle=\"--\")\n",
    "ax1.text(df_test.iloc[DIST_WINDOW][\"sod\"] + 80, np.max(df_assess[\"absolute_error\"].values) / 2, \"Window Buffer\", rotation=90, verticalalignment='center', color=\"purple\")\n",
    "\n",
    "plt.subplot(grid[0, 0:8])\n",
    "ax2 = sns.heatmap(\n",
    "    [df_test[\"dist_measure\"].values],\n",
    "    cmap=\"bwr\",\n",
    "    cbar=False,\n",
    "    xticklabels=False,\n",
    "    yticklabels=False,\n",
    "    vmin=0, \n",
    "    vmax=VMAX\n",
    ")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assess_dist_calc = calc_dist_measure(\n",
    "    df_assess,\n",
    "    window_size=DIST_WINDOW,\n",
    "    show_dist_plot=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assess[\"dist_measure\"] = [0.] * df_assess.shape[0]\n",
    "df_assess[\"dist_measure\"] = df_assess[\"sod\"].apply(\n",
    "    lambda x: get_val(df_assess_dist_calc, x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_assess[\"dist_measure\"].values, kde=True, rug=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assess[\"dist_measure\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assess.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_assess = loop.LocalOutlierProbability(\n",
    "    df_assess.iloc[BATCH_SIZE:][\"absolute_error\"].values, \n",
    "    extent=3, \n",
    "    n_neighbors=30\n",
    ").fit()\n",
    "m_assess_scores = m_assess.local_outlier_probabilities\n",
    "df_assess[\"loop_scores\"] = list([0.] * BATCH_SIZE) + list(m_assess_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(40, 10))\n",
    "grid = plt.GridSpec(4, 8, wspace=0.4, hspace=0.3)\n",
    "\n",
    "plt.subplot(grid[1:3, 0:8])\n",
    "\n",
    "# ax1 = plt.figure(figsize=(30, 10))\n",
    "ax1 = sns.lineplot(x=df_assess[\"sod\"], y=\"absolute_error\", data=df_assess, color=\"black\")\n",
    "ax1.margins(x=0)\n",
    "\n",
    "# add a vertical line to indicate the event \n",
    "ax1.axvline(sod_annotations[SAT], color=\"blue\", linestyle=\"--\")\n",
    "ax1.text(sod_annotations[SAT] + 80, np.max(df_assess[\"absolute_error\"].values) / 2, \"Start of Event\", rotation=90, verticalalignment='center', color=\"blue\")\n",
    "\n",
    "# add a vertical line to indicate when a full window of distance values is available \n",
    "ax1.axvline(df_assess.iloc[DIST_WINDOW][\"sod\"], color=\"purple\", linestyle=\"--\")\n",
    "ax1.text(df_assess.iloc[DIST_WINDOW][\"sod\"] + 80, np.max(df_assess[\"absolute_error\"].values) / 2, \"Window Buffer\", rotation=90, verticalalignment='center', color=\"purple\")\n",
    "\n",
    "\n",
    "plt.subplot(grid[0, 0:8])\n",
    "ax2 = sns.heatmap(\n",
    "    [df_assess[\"dist_measure\"].values],\n",
    "    cmap=\"bwr\",\n",
    "    cbar=False,\n",
    "    xticklabels=False,\n",
    "    yticklabels=False,\n",
    "    vmin=0, \n",
    "    vmax=VMAX\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Model and Approach to Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to report the following metrics: \n",
    "\n",
    "- **Root Mean Square Error**: measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed\n",
    "- **Accuracy**: the number of correct classifications over the number of observations\n",
    "- **Recall**: fraction of true events that were detected\n",
    "- **Precision**: fraction of detections reported  by the model that are correct\n",
    "- **F-Score**: the harmonic mean of the precision and recall, `2pr / (p + r)`\n",
    "- **Coverage**: fraction of examples for which the system is able to produce a confident classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-tsunami",
   "language": "python",
   "name": "venv-tsunami"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
